#!/usr/bin/env python
# coding: utf-8

# # Выпускная квалификационная работа Шортановой Регины Исфандияровны

# Cлушателя курса "Data Science" Образовательного центра Московского государственного технического университета им. Н.Э. Баумана

# ## Тема иследования: Прогнозирование конечных свойств новых материалов (композиционных материалов). 
# ## Цель исследования: построение моделей прогнозирования следующих параметров:
# ### - Модуль упругости при растяжении;
# ### - Прочность пр растяжении;
# ### - Соотношение матрица - наполнитель
# ## Итог исследования: Разработка приложения с графическим интерфейсом, которое будет выдавать прогноз параметра "соотношение матрица - наполнитель"
Приложение 1
Подробный план работы:
1.	Загрузить и обработать входящие датасеты
1.1.	Удалить неинформативные столбцы
1.2.	Объединить датасеты по методу INNER
2.	Провести разведочный анализ данных:
2.1.	Данные в столбце "Угол нашивки» привести к 0 и 1
2.2.	Изучить описательную статистику каждой переменной - "среднее", "медиана", "стандартное отклонение", "минимум", "максимум", "квартили"
2.3.	Проверить датасет на пропуски и дубликаты данных
2.4.	Получить среднее, медианное значение для каждой колонки
2.5.	Вычислить коэффициенты ранговой корреляции Кендалла
2.6.	Вычислить коэффициенты корреляции Пирсона
3.	Визуализировать разведочный анализ сырых данных (до выбросов и нормализации)
3.1.	Построить несколько вариантов гистограмм распределения каждой переменной
3.2.	Построить несколько вариантов диаграмм "ящиков с усами" каждой переменной
3.3.	Построить гистограмму распределения и диаграмма "ящик с усами" одновременно вместе с данными по каждому столбцу
3.4.	Построить несколько вариантов попарных графиков рассеяния точек (матрицы диаграмм рассеяния) 
3.5.	Построить графики "квантиль-квантиль"
3.6.	Построить корреляционную матрицу с помощью тепловой карты
4.	Провести предобработку данных (в данном пункте - очистка датасета от выбросов) 
4.1.	Проверить выбросы по 2 методам: 3-х сигм или межквартильных расстояний 
4.2.	Посчитать распределение выбросов по каждому столбцу (с целью предотвращения удаления особенностей признака или допущения ошибки)
4.3.	Исключить выбросы методом межквартильного расстояния
4.4.	Удалить строки c выбросами
4.5.	Визуализировать датасет без выбросов, и убедиться, что выбросы еще есть.
4.6.	Для полной очистки датасета от выбросов повторить пункты (4.3 – 4.5) ещё 3 раза. 
4.7.	Сохранить идеальный, без выбросов датасет
4.8.	Изучить чистые данные по всем параметрам
4.9.	Визуализировать «чистый» датасет (без выбросов)
5.	Провести нормализацию и стандартизацию
5.1.	Визуализировать плотность ядра
5.2.	Нормализовать данные с помощью MinMaxScaler()
5.3.	Нормализовать данные с помощью Normalizer()
5.4.	Сравнить с данными до нормализации
5.5.	Проверить перевод данных из нормализованных в исходные
5.6.	Рассмотреть несколько вариантов корреляции между параметрами после нормализации
5.7.	Стандартизировать данные
5.8.	Визуализировать данные корреляции
5.9.	Посмотреть на описательную статистику после нормализации и после стандартизации
6.	Разработать и обучить нескольких моделей прогноза прочности при растяжении 
6.1.	Определить входы и выходы для моделей
6.2.	Разобить данные на обучающую и тестовую выборки
6.3.	Проверить правильность разбивки
6.4.	Построить модели и найти лучшие гиперпараметры
6.5.	Построить и визуализировать результат работы метода "опорных векторов"
6.6.	Построить и визуализировать результат работы метода "случайного леса"
6.7.	Построить и визуализировать результат работы линейной регрессии
6.8.	Построить и визуализировать результат работы метода "градиентного бустинга"
6.9.	Построить и визуализировать результат работы метода "К ближайших соседей"
6.10.	Построить и визуализировать результат работы метода "деревья решений"
6.11.	Построить и визуализировать результат работы стохастического градиентного спуска
6.12.	Построить и визуализировать результат работы многослойного перцептрона
6.13.	Построить и визуализировать результат работы лассо регрессии
6.14.	Сравнить модели по метрике МАЕ
6.15.	Найти лучшие гиперпараметры для "случайного леса"
6.16.	Подставить значения в модель "случайного леса"
6.17.	Найти лучшие гиперпараметры для "К ближайших соседей"
6.18.	Подставить значения в модель "К ближайших соседей
6.19.	Найти лучшие гиперпараметры метода "деревья решений"
6.20.	Подставить значения в модель метода "деревья решений"
6.21.	Проверить все модели и процессинги и вывести лучшую модель и процессинг
7.	Разработать и обучить нескольких моделей прогноза модуля упругости при растяжении
7.1.	Определить входы и выходы для моделей
7.2.	Разобить данные на обучающую и тестовую выборки
7.3.	Проверить правильность разбивки
7.4.	Построить модели и найти лучшие гиперпараметры
7.5.	Построить и визуализировать результат работы метода "опорных векторов"
7.6.	Построить и визуализировать результат работы метода "случайного леса"
7.7.	Построить и визуализировать результат работы линейной регрессии
7.8.	Построить и визуализировать результат работы метода градиентного бустинга
7.9.	Построить и визуализировать результат работы метода "К ближайших соседей"
7.10.	Построить и визуализировать результат работы метода "деревья решений"
7.11.	Построить и визуализировать результат работы стохастического градиентного спуска
7.12.	Построить и визуализировать результат работы многослойного перцептрона
7.13.	Построить и визуализировать результат работы лассо регрессии
7.14.	Сравнить модели по метрике МАЕ
7.15.	Найти лучшие гиперпараметры для случайного леса
7.16.	Подставить значения в модель "случайного леса"
7.17.	Найти лучшие гиперпараметры для "К ближайших соседей"
7.18.	Подставить значения в модель "К ближайших соседей"
7.19.	Найти лучшие гиперпараметры метода "деревья решений"
7.20.	Подставить значения в модель метода "деревья решений"
7.21.	Проверить все модели и процессинги и вывести лучшую модель и процессинг
8.	Нейронная сеть для рекомендации соотношения матрица-наполнитель
8.1.	Сформировать входы и выход для модели
8.2.	Нормализовать данные
8.3.	Построить модель, определить параметры
8.4.	Найти оптимальные параметры для модели
8.5.	Посмотреть на результаты
8.6.	Повторить шаги 8.4 – 8.5 до построения окончательной модели
8.7.	Обучить нейросеть 80/20
8.8.	Оценить модель
8.9.	Посмотреть на потери модели
8.10.	Посмотреть на график результата работы модели
8.11.	Посмотреть на график потерь на тренировочной и тестовой выборках
8.12.	Сконфигурировать другую модель, задать слои
8.13.	Посмотреть на архитектуру другой модели
8.14.	Обучить другую модель
8.15.	Посмотреть на потери другой модели
8.16.	Посмотреть на график потерь на тренировочной и тестовой выборках
8.17.	Задать функцию для визуализации факт/прогноз для результатов моделей
8.18.	Посмотреть на график результата работы модели
8.19.	Оценить модель MSE
# In[1]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns
import plotly.express as px
import tensorflow as tf
import sklearn

from sklearn import linear_model
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression, SGDRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn import preprocessing
from sklearn.preprocessing import Normalizer, LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

from tensorflow import keras as keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Activation
from pandas import read_excel, DataFrame, Series
from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor
from tensorflow.keras.models import Sequential
from numpy.random import seed
from scipy import stats
import warnings
warnings.filterwarnings("ignore")


# ## 1.Загрузить и проработать входящие датасеты

# In[2]:


#1-Загружаем первый датасет (базальтопластик) и посмотрим на названия столбцов
df_bp = pd.read_excel(r"C:\Users\user\Desktop\МГТУ учеба\vkr\datasets\X_bp.xlsx")
df_bp.shape


# ### 1.1 Удаляем первый неинформативный столбец 

# In[3]:


df_bp.drop(['Unnamed: 0'], axis=1, inplace=True)
#Посмотрим на первые 10 строк первого датасета и убедимся, что первый столбец удалился
df_bp.head(10)


# In[4]:


# Проверим размерность первого файла
df_bp.shape


# In[5]:


#Загружаем второй датасет (углепластик) 
df_nup = pd.read_excel(r"C:\Users\user\Desktop\МГТУ учеба\vkr\datasets\X_nup.xlsx")
df_nup.shape


# In[6]:


#Удаляем первый неинформативный столбец
df_nup.drop(['Unnamed: 0'], axis=1, inplace=True)
#Посмотрим на первые 10 строк второго датасета и убедимся, что и здесь не нужный первый столбец успешно удалился
df_nup.head(10)


# In[7]:


# Проверим размерность второго файла
df_nup.shape


# ## 1.2 Объединить датасеты по методу INNER

# In[8]:


# Представленные датасеты имеют разный объем строк. 
# Собрать исходные данные файлы в один, единый набор данных.  
df = df_bp.merge(df_nup, left_index = True, right_index = True, how = 'inner')
df.head().T


# In[9]:


#Посмотрим количество колонок и столбцов
df.shape
# Итоговый датасет имеет 13 столбцов и 1023 строки, 17 строк из таблицы X_nup было отброшено,
#т.е часть данных удалена на начальном этапе исследования.


# ## 2.Разведочный анализ данных

# In[10]:


# Посмотрим на начальные и конечные строки нашего датасета на данном этапе работы
df


# In[11]:


#Просмотрим информацию о датасете, проверим тип данных в каждом столбце (типы признаков)
df.info()
# Пропусков не имеется.Ни одна из записей не является NaN, очистка не требуется. Объединенный файл имеет всего 1023 строки.


# In[12]:


#Применем функцию nunique для поиска уникальных значений
df.nunique()
#В основном общее число уникальных значений в каждом столбце, но в столбце "Угол нашивки" всего 2 значения. 
#Проработаем их.


# In[13]:


df['Угол нашивки, град'].nunique()
#Так как кол-во уникальных значений в колонке "Угол нашивки" равно 2, можем привести данные в этой колонке к значениям 0 и 1


# In[14]:


#Проверим кол-во элементов, где Угол нашивки равен 0 градусов
df['Угол нашивки, град'][df['Угол нашивки, град'] == 0.0].count()


# ### 2.1 Приведем столбец "Угол нашивки" к значениям 0 и 1 и integer

# In[15]:


df = df.replace({'Угол нашивки, град': {0.0 : 0, 90.0 : 1}})
df['Угол нашивки, град'] = df['Угол нашивки, град'].astype(int)


# In[16]:


#Переименуем столбец
df = df.rename(columns={'Угол нашивки, град' : 'Угол нашивки'})
df


# In[17]:


#Посчитаем количество элементов, где угол нашивки равен 0 градусов
df['Угол нашивки'][df['Угол нашивки'] == 0.0].count()
#После преобразования колонки Угол нашивки к значениям 0 и 1, кол-во элементов, где угол нашивки равен 0 не изменилось 


# In[18]:


# Переведем столбец с нумерацией в integer
df.index = df.index.astype('int')


# In[19]:


# Сохраним итоговый датасет в отдельную папку с данными
df.to_excel(r'C:\Users\user\Desktop\МГТУ учеба\vkr\itogoviidataset\itogoviidataset.xlsx')


# ### 2.2 Изучим описательную статистику каждой переменной - среднее, медиана, стандартное отклонение, минимум, максимум, квартили 

# In[20]:


#Посмотрим на основные параметры анализа данных
df.describe()


# In[21]:


a = df.describe()
a.T


# In[22]:


#Описательная статистика содержит по каждому столбцу (по каждой переменной):
#count - количество значений
#mean - среднее значение
#std - стандартное отклонение
#min - минимум
#25% - верхнее значение первого квартиля
#50% - медиана
#75% - верхнее значение третьего квартиля
#max - максимум


# ### 2.3 Проверим датасет на пропуски и дубликаты данных

# In[23]:


df.isnull().sum()
# Пропущенных данных нет = нулевых значений нет


# In[24]:


#светло-зеленый - не пропущенные, темнозеленый - пропущенные данные
cols = df.columns
colours = ['#ceff1d', '#008000'] 
sns.heatmap(df[cols].isnull(), cmap = sns.color_palette(colours))
#Тепловая карта, так же как info() и функция ISNULL() показывает, что пропусков нет.


# In[25]:


for col in df.columns:
    pct_missing = np.mean(df[col].isnull())
    print('{} - {}%'.format(col, round(pct_missing*100)))


# In[26]:


df.duplicated().sum()
#Дубликатов нет


# ### 2.4 Получим среднее, медианное значение для каждой колонки (по заданию необходимо получить их отдельно, поэтому продублируем их только отдельно)

# In[27]:


#получим среднее и медианное значения данных в колонках
mean_and_50 = df.describe()
mean_and_50.loc[['mean', '50%']]
#в целом мы видим близкие друг к другу значения


# In[28]:


#Среднее значение
df.mean()


# In[29]:


#Медианное значение
df.median()


# ### 2.5 Вычислим коэффициенты ранговой корреляции Кендалла 

# In[30]:


df.corr(method = 'kendall')
#Cтатической зависимости не наблюдаем


# ### 2.6	Вычислим коэффициенты корреляции Пирсона 

# In[31]:


df.corr(method ='pearson')
#Статистической зависимости не наблюдаем


# In[32]:


#Создадим переменную для названия всех столбцов. Это нам пригодится при построении моделей. И перейдем к визуализации данных
df.columns
#column_names = ["Соотношение матрица-наполнитель","Плотность, кг/м3","модуль упругости, ГПа","Количество отвердителя, м.%",
#         "Содержание эпоксидных групп,%_2","Температура вспышки, С_2","Поверхностная плотность, г/м2",
#         "Модуль упругости при растяжении, ГПа","Прочность при растяжении, МПа","Потребление смолы, г/м2",
#        "Угол нашивки, град","Шаг нашивки","Плотность нашивки"]
column_names = df.columns


# ## 3. Визуализируем сырые данные и проведем анализ

# ### 
# -Построим гистограммы распределения каждой из переменных и боксплоты (несколько разных способов визуализации);
# 
# -диаграммы "ящиков с усами" (несколько вариантов);
# 
# -попарные графики рассеяния точек (несколько вариантов);
# 
# -графики квантиль-квантиль без нормализации и исключения шумов.
# 
#  Используем разные варианты визуализации, для получения какой-либо корреляций. И разные варианты одного и того же типа визуализации используются для отображения результата, потому что какие-то графики отображаются в jupiter, но не работают в colab, какие-то не работают в Github
#  
#  Показатели описательной статистики и визуализация гистограмм и/или диаграмм размаха («ящик с усами») позволяют получить наглядное представление о характерах распределений переменных. Такое частотное распределение показывает, какие именно конкретные значения или диапазоны значений исследуемой переменной встречаются наиболее часто, насколько различаются эти значения, распо-ложено ли большинство наблюдений около среднего значения, является распределение симметричным или асимметричным, многомодальным (т.е. имеет две или более вершины) или одномодальным и т.д. По форме распределения можно судить о природе исследуемой переменной (например, бимодальное распределение позволяет предположить, что выборка не является однородной и содержит наблюдения, принадлежащие двум различным множествам, которые в свою очередь нормально распределены).

# In[33]:


# Построим гистограммы распределения каждой из переменных без нормализации и исключения шумов
df.hist(figsize = (20,20), color = "g")
plt.show()


# ###
# При проведении анализа выявлены параметры близкие к нормальному:
# 
# -Соотношение матрица-наполнитель; 
# 
# -Плотность, кг/м3;
# 
# -Модуль упругости, Гпа; 
# 
# -Количество отвердителя, м.%;
# 
# -Содержание эпоксидных групп,%_2;
# 
# -Температура вспышки, С_2;
# 
# -Поверхностная плотность, г/м2;
# 
# -Модуль упругости при растяжении, Гпа;
# 
# -Прочность при растяжении, Мпа;
# 
# -Потребление смолы, г/м2;
# 
# -Шаг нашивки; 
# 
# -Плотность нашивки. 
# 
# Преимущественно данные стремятся к нормальному распределению. Угол нашивки, как и отражено в датасете, имеет только два значения 90 градусов и 0 градусов, что отражает общий подход к проведению нашивки материалов, а также может быть использовано при обработке данных. Учитывая отсутствие иных показателей для угла нашивки, предлагаем в прогнозе использовать категориальный, а не непрерывный подход при анализе данного параметра.

# In[34]:


# Гистограмма распределения (второй вариант)
a = 5 # количество строк
b = 5 # количество столцбцов
c = 1 # инициализация plot counter
plt.figure(figsize = (35,35))
plt.suptitle('Гистограммы переменных', fontsize = 30)
for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.histplot(data = df[col], kde=True, color = "darkgreen")
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c += 1
#Гистограммы показывают ярковыраженные выбросы в столбцах:
#плотность, содержание эпоксидных групп, температура вспышки, плотность нашивки. 
#Данные стремятся к нормальному распределению практически везде, кроме угла нашивки, имеющим только 2 значения


# In[35]:


# гистограмма распределения и боксплоты (третий вариант)

for column in df.columns:
    fig = px.histogram(df, x = column, color_discrete_sequence = ['green'], nbins = 100, marginal = "box")
    fig.show()


# In[36]:


for column in df.columns:
    fig = px.box(df, y = column)
    fig.show()


# ## 3.2	Построим несколько вариантов диаграмм "ящиков с усами" каждой переменной 

# In[37]:


# "Ящики с усами"(боксплоты) (первый вариант)
scaler = MinMaxScaler()
scaler.fit(df)
plt.figure(figsize = (20, 20))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,
             fontsize = 30)
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df.columns,patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'g', color = 'y'),medianprops = dict(color = 'lime'), whiskerprops = dict(color="g"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()


# ### 
# Многие алгоритмы машинного обучения чувствительны к разбросу и распределению значений признаков обрабатываемых объектов. Соответственно, выбросы во входных данных могут исказить и ввести в заблуждение процесс обучения алгоритмов машинного обучения, что приводит к увеличению времени обучения, снижению точности моделей и, в конечном итоге, к снижению результатов. 

# In[38]:


# Ящики с усами (второй вариант)
a = 5 # количество строк
b = 5 # количество столцбцов
c = 1 # инициализация plot counter

plt.figure(figsize = (35,35))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,
             fontsize = 30)
for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.boxplot(data = df, y = df[col], fliersize = 15, linewidth = 5, boxprops = dict(facecolor = 'y', color = 'g'), medianprops = dict(color = 'lime'), whiskerprops = dict(color="g"), capprops = dict(color = "yellow"), flierprops = dict(color="y", markeredgecolor = "lime"))
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c += 1
# "Ящики с усами" показывают наличие выбросов во всех столбцах, кроме углов нашивки, значит, с ними будем работать


# ## 3.3 	Построим гистограмму распределения и диаграмма "ящик с усами" одновременно вместе с данными по каждому столбцу 

# In[39]:


for column_name in column_names:
    print(column_name)
    
    #Гистограмма распределения
    gis = df[column_name]
    sns.set_style("whitegrid")
    sns.kdeplot(data = gis, shade = True, palette ='colorblind', color = "g")
    plt.show()
    
    #Диаграмма "Ящик с усами"
    sns.boxplot(x=gis, color = "g");
    plt.show()
    
    #Значения (мин макс ср)
    print("Минимальное значение: ", end = " ")
    print(np.min(gis))
    print("Максимальное значение: ", end=" ")
    print(np.max(gis))
    print("Среднее значение: ", end = " ")
    print(np.mean(gis))

    print("Медианное значение: ", end = " ")
    print(np.median(gis))
    print("\n\n")
# Кроме "Угол нашивки, град" и "Поверхностная плотность, г/м2" остальные переменные относительно хорошо соответствуют нормальному распределению


# ## 3.4	Построим несколько вариантов попарных графиков рассеяния точек (матрицы диаграмм рассеяния)  

# In[40]:


# Попарные графики рассеяния точек (матрица диаграмм рассеяния) (первый вариант)
sns.set_style('darkgrid')
sns.pairplot(df, hue = 'Угол нашивки', markers = ["o", "s"], diag_kind = 'auto', palette='YlGn')
# Попарные графики рассеяния точек так же не показывают какой-либо зависимости между данными.
#Зависимость между показателями не линейная, взаимосвязь отсутствует, необходимо использовать несколько показателей. 
# из графиков можно наблюдать выбросы, потому что некоторые точки располагаются далеко от общего облака


# In[41]:


# Попарные графики рассеяния точек - скаттерплоты (второй вариант) 
g = sns.PairGrid(df[df.columns])
g.map(sns.scatterplot, color = 'darkgreen')
g.map_upper(sns.scatterplot, color = 'darkgreen')
g.map_lower(sns.kdeplot, color = 'darkgreen')
plt.show
# Корреляции нет


# ## 3.5	Построим графики квантиль-квантиль 

# In[42]:


for i in df.columns:
    plt.figure(figsize = (6, 4))
    res = stats.probplot(df[i], plot = plt)
    plt.title(i, fontsize = 10)
    plt.xlabel("Теоретические квантили", fontsize = 10)
    plt.ylabel("Упорядоченные значения", fontsize = 10)
    plt.show()


# ## 3.6	Построим корреляционную матрицу с помощью тепловой карты 

# In[43]:


mask = np.triu(df.corr())
# Создаем полотно для отображения большого графика
f, ax = plt.subplots(figsize = (11, 9))
# # Визуализируем данные кореляции и создаем цветовую палитру
sns.heatmap(df.corr(), mask = mask, annot = True, square = True, cmap = 'YlGn')
plt.xticks(rotation = 45, ha='right')
plt.show()
# Максимальная корреляция между Плотностью нашивки и углом нашивки и составляет 0.11,
#что говорит об отсутствии зависимости между этими данными. 
# Корреляция между всеми параметрами очень близка к 0, что говорит об отсутствии корреляционных связей между переменными.


# In[44]:


# График корреляции подтверждает данные теории композитных материалов.
# Мы видим, что на качество материла влияет температура вспышки и количество отвердителя из-за взаимодействия отвердителя с матрицей и наполнителем под влиянием температуры.
# Угол нашивки и плотность нашивки несомненно оказывают влияние на свойства материала. 
# А потребление смолы и соотношение матрицы-наполнителя, плотности и плотности нашивки, модуля упругости и плотности нашивки имеют не особенно выраженную корреляцию.


# ### Вывод на данном этапе работы: На наших "сырых" данных мы наблюдаем выбросы в каждом столбце, кроме столбца "Угол нашивки" и корреляция входных переменных очень слабая.

# # 4.	Проведём предобработку данных  

# In[45]:


#Так как пропуски в данных отсутствуют, можем сразу приступить к работе с выбросами
# Шаг 1 Удаление выбросов. Посчитаем, сколько значений у нас в каждом столбце выбивающихся из распределения.
df.isna().sum()
#Т.к.значений не очень много, их вполне можно исключить.


# ## 4.1	Проверим выбросы по 2 методам: 3-х сигм или межквартильных расстояний 

# In[46]:


#Сравним эти 2 метода.
metod_3s = 0
metod_iq = 0
count_iq = [] # Список, куда записывается количество выбросов по каждой колонке датафрейма методом.
count_3s = [] # Список, куда записывается количество выбросов по каждой колонке датафрейма.
for column in df:
    d = df.loc[:, [column]]
    # методом 3-х сигм
    zscore = (df[column] - df[column].mean()) / df[column].std()
    d['3s'] = zscore.abs() > 3
    metod_3s += d['3s'].sum()
    count_3s.append(d['3s'].sum())
    print(column,'3s', ': ', d['3s'].sum())

    # методом межквартильных расстояний
    q1 = np.quantile(df[column], 0.25)
    q3 = np.quantile(df[column], 0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    d['iq'] = (df[column] <= lower) | (df[column] >= upper)
    metod_iq += d['iq'].sum()
    count_iq.append(d['iq'].sum())
    print(column, ': ', d['iq'].sum())
print('Метод 3-х сигм, выбросов:', metod_3s)
print('Метод межквартильных расстояний, выбросов:', metod_iq)


# ## 4.2	Посчитаем распределение выбросов по каждому столбцу (с целью предотвращения удаления особенностей признака или допущения ошибки)

# In[47]:


#С целью предотвращения удаления особенностей признака или допущения ошибки, посчитаем распределение выбросов по каждому столбцу.
m = df.copy()
for i in df.columns:
    m[i] = abs((df[i] - df[i].mean()) / df[i].std())
    print(f"{sum(m[i] > 3)} выбросов в признаке {i}")
print(f' Всего {sum(sum(m.values > 3))} выброса')


# In[48]:


#Создадим переменную со списком всех параметров, в которых есть выбросы
df.columns
column_list_drop = ["Соотношение матрица-наполнитель",
                 "Плотность, кг/м3",
                 "модуль упругости, ГПа",
                 "Количество отвердителя, м.%",
                 "Содержание эпоксидных групп,%_2",
                 "Температура вспышки, С_2",
                 "Поверхностная плотность, г/м2",
                 "Модуль упругости при растяжении, ГПа",
                 "Прочность при растяжении, МПа",
                 "Потребление смолы, г/м2",
                 "Шаг нашивки",
                 "Плотность нашивки"]


# ## 4.3	Исключим выбросы методом межквартильного расстояния 

# In[49]:


# Oчистим данные от выбросов методом межквартильного расстояния (далее 1,5 межквартильных размахов)
# Выбор сделан в пользу этого метода, потому что хотим добиться в данной работе полностью избавления от выборов, 
#и метод межквартильного расстояния позволяет удалить практически 10% датасета сразу
for i in column_list_drop:
    q75, q25 = np.percentile(df.loc[:,i], [75,25])
    intr_qr = q75 - q25
    max = q75 + (1.5 * intr_qr)
    min = q25 - (1.5 * intr_qr)
    df.loc[df[i] < min, i] = np.nan
    df.loc[df[i] > max, i] = np.nan


# ## 4.4	Удалим строки c выбросами

# In[50]:


# Так же для удаления выбросов можно использовать вот эту формулу, но мне привычней вариант выше:
#df = df[~((df<(min))|(df>(max))).any(axis=1)]
#df
# Можно так же удалить выборосы методом 3-х сигм, но мы не будем.
m_3s = pd.DataFrame(index=df.index)
for column in df:
    zscore = (df[column] - df[column].mean()) / df[column].std()
    m_3s[column] = (zscore.abs() > 3)
df = df[m_3s.sum(axis=1)==0]
df.shape


# In[51]:


#Посмотрим на сумму выбросов по каждому из столбцов
df.isnull().sum()
#Всего 64 выброса, можно их удалить.


# In[52]:


#Удаляем строки c выбросами
df = df.dropna(axis=0)


# In[53]:


#И еще раз посмотрим на сумму выбросов по каждому из столбцов, чтобы убедиться, что все работает
df.isnull().sum()


# In[54]:


#Просмотрим информацию о "чистом" датасете после удаления пропусков. Видим, что строк стало меньше
df.info()
# После удаления выбросов в датасете осталось 936 строк и 13 колонок


# In[55]:


# Ящики с усами (второй вариант)
a = 5 # количество строк
b = 5 # количество столцбцов
c = 1 # инициализация plot counter

plt.figure(figsize = (35,35))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,
             fontsize = 30)
for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.boxplot(data = df, y = df[col], fliersize = 15, linewidth = 5, boxprops = dict(facecolor = 'y', color = 'g'), medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color="yellow"), flierprops = dict(color = "y", markeredgecolor = "lime"))
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c += 1


# In[56]:


#Построим "ящики с усами" и наблюдаем все еще наличие выбросов
scaler = MinMaxScaler()
scaler.fit(df)
plt.figure(figsize = (20, 20))
#Посмотрим на  "ящики с усами",чтобы наглядно увидеть, что выбросов нет, но они есть
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df.columns,patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'g', color = 'y'),medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()


# In[57]:


# На графиках выше мы видим выбросы в некоторых столбцах. Они всё ещё есть, поэтому повторяем удаление выбросов

for i in column_list_drop:
    q75, q25 = np.percentile(df.loc[:,i],[75, 25])
    intr_qr = q75 - q25
    max = q75 + (1.5 * intr_qr)
    min = q25 - (1.5 * intr_qr)
    df.loc[df[i] < min, i] = np.nan
    df.loc[df[i] > max, i] = np.nan


# In[58]:


# Ящики с усами (второй вариант)
a = 5 # количество строк
b = 5 # количество столцбцов
c = 1 # инициализация plot counter

plt.figure(figsize=(35,35))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,
             fontsize = 30)
for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.boxplot(data = df, y = df[col], fliersize = 15, linewidth = 5, boxprops = dict(facecolor = 'y', color = 'g'), medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color="yellow"), flierprops = dict(color = "y", markeredgecolor = "lime"))
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c += 1


# In[59]:


df.skew()#ассиметрия по всем колонкам. 


# In[60]:


df.kurt()# эксцесс по всем колонкам. 


# In[61]:


#Проверим сумму выбросов по каждому из столбцов
df.isnull().sum()


# In[62]:


#И снова удаляем строки, которые содержат выбросы
df = df.dropna(axis=0)


# In[63]:


#Третий раз проверим сумму выбросов по каждому столбцу
df.isnull().sum()


# In[64]:


#Просмотрим информацию о нашем датасете после еще одного удаления пропусков. Видим, что строк стало еще меньше
df.info()


# In[65]:


#В третий раз построим на "ящики с усами"
scaler = MinMaxScaler()
scaler.fit(df)
plt.figure(figsize = (20, 20))
#Выводим "ящики"
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df.columns,patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'g', color = 'y'),medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()


# In[66]:


# Ящики с усами (второй вариант)
a = 5 # количество строк
b = 5 # количество столцбцов
c = 1 # инициализация plot counter

plt.figure(figsize = (35,35))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,
             fontsize = 30)
for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.boxplot(data = df, y = df[col], fliersize = 15, linewidth = 5, boxprops = dict(facecolor = 'y', color = 'g'), medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color = "yellow"), flierprops = dict(color = "y", markeredgecolor = "lime"))
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c += 1


# In[67]:


# И снова видим, что выбросы остались в некоторых исходных данных.


# In[68]:


# Повторяем процедуру с выбросами еще раз. 
for i in column_list_drop:
    q75, q25 = np.percentile(df.loc[:,i],[75, 25])
    intr_qr = q75 - q25
    max = q75 + (1.5*intr_qr)
    min = q25 - (1.5*intr_qr)
    df.loc[df[i] < min,i] = np.nan
    df.loc[df[i] > max,i] = np.nan


# In[69]:


#Еще раз проверим сумму выбросов в каждом столбце
df.isnull().sum()


# In[70]:


#Еще раз удаляем строки с выбросами
df = df.dropna(axis=0)


# In[71]:


#И проверочно посмотрим сумму выбросов
df.isnull().sum()


# ## 4.9	Визуализируем «чистый» датасет 

# In[72]:


df.info()


# In[73]:


# "Ящики с усами"(боксплоты) (первый вариант)
scaler = MinMaxScaler()
scaler.fit(df)
plt.figure(figsize = (20, 20))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,
             fontsize = 30)
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df.columns,patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'g', color = 'y'),medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()


# In[74]:


# Ящики с усами (второй вариант)
a = 5 # количество строк
b = 5 # количество столцбцов
c = 1 # инициализация plot counter

plt.figure(figsize = (35,35))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,
             fontsize = 30)
for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.boxplot(data = df, y = df[col], fliersize = 15, linewidth = 5, boxprops = dict(facecolor = 'y', color = 'g'), medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color = "yellow"), flierprops = dict(color = "y", markeredgecolor = "lime"))
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c += 1


# In[75]:


# Построим гистограммы распределения каждой из переменных без нормализации
df.hist(figsize = (20,20), color = "g")
plt.show()


# In[76]:


# Гистограмма распределения (второй вариант)
a = 5 # количество строк
b = 5 # количество столцбцов
c = 1 # инициализация plot counter
plt.figure(figsize=(35,35))
plt.suptitle('Гистограммы переменных', fontsize = 30)
for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.histplot(data = df[col], kde = True, color = "darkgreen")
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c += 1
#Данные стремятся к нормальному распределению практически везде, кроме угла нашивки, имеющим только 2 значения, 
#с которым мы уже поработали ранее.


# In[77]:


# гистограмма распределения и боксплоты (третий вариант)

for column in df.columns:
    fig = px.histogram(df, x=column, color_discrete_sequence=['green'], nbins=100, marginal="box")
    fig.show()


# In[78]:


for column in df.columns:
    fig = px.box(df, y=column)
    fig.show()


# In[79]:


# Ящики с усами на одном рисунке
plt.figure(figsize=(16,10))
ax = sns.boxplot(data=df)
ax.set_xticklabels(ax.get_xticklabels(),rotation=30);


# In[80]:


#часть кода выше не работает в колабе, поэтому выбросы проверим еще другими способами
for column_name in column_names:
  print(column_name)
    
    #Гистограмма распределения
  gis = df[column_name]
  sns.set_style("whitegrid")
  sns.kdeplot(data = gis, shade = True, palette = 'colorblind', color = "g")
  plt.show()
    
    #Диаграмма "Ящик с усами"
  sns.boxplot(x = gis, color = "g");
  plt.show()
    
    #Значения (мин макс ср)
  print("Минимальное значение: ", end = " ")
  print(np.min(gis))
  print("Максимальное значение: ", end = " ")
  print(np.max(gis))
  print("Среднее значение: ", end = " ")
  print(np.mean(gis))

  print("Медианное значение: ", end = " ")
  print(np.median(gis))
  print("\n\n")


# In[81]:


n = 0
while n < len(column_names):
    b = n + 1
    while b < len(column_names):
        sns.set_style('whitegrid')
        plt.title('Зависимость',size = 16)
        plt.xlabel(column_names[n],size = 12)
        plt.ylabel(column_names[b],size = 12)
        sns.scatterplot(x = column_names[n], y = column_names[b], data = df, color = "green", edgecolor = 'lime', palette = 'cubehelix')
        plt.show()
        b += 1
    n += 1


# In[82]:


sns.set_style('darkgrid')
sns.pairplot(df, hue = 'Угол нашивки', markers = ["o", "s"], diag_kind = 'auto', palette = 'YlGn')


# In[83]:


# Попарные графики рассеяния точек - скаттерплоты (второй вариант) 
g = sns.PairGrid(df[df.columns])
g.map(sns.scatterplot, color = 'darkgreen')
g.map_upper(sns.scatterplot, color = 'darkgreen')
g.map_lower(sns.kdeplot, color = 'darkgreen')
plt.show
# Корреляции нет


# In[84]:


for i in df.columns:
    plt.figure(figsize = (6, 4))
    res = stats.probplot(df[i], plot = plt)
    plt.title(i, fontsize = 10)
    plt.xlabel("Теоретические квантили", fontsize = 10)
    plt.ylabel("Упорядоченные значения", fontsize = 10)
    plt.show()


# In[85]:


#Визуализация корреляционной матрицы с помощью тепловой карты
mask = np.triu(df.corr())
# Создаем полотно для отображения большого графика
f, ax = plt.subplots(figsize=(11, 9))
# # Визуализируем данные кореляции и создаем цветовую политру
sns.heatmap(df.corr(), mask = mask, annot = True, square = True, cmap = 'YlGn')
plt.xticks(rotation = 45, ha = 'right')
plt.show()


# In[86]:


#В итоге осталось всего 922 строки 
df.info()


# In[87]:


#Сохраняем , чтобы в excel проверить дополнительно
df.to_csv(r"C:\Users\user\Desktop\МГТУ учеба\vkr\itogoviidataset\itogoviidataset.csv", encoding = "cp1251")
df.to_excel(r"C:\Users\user\Desktop\МГТУ учеба\vkr\itogoviidataset\itogoviidataset.xlsx")


# In[88]:


#Проводим повторный разведочный анализ уже без выбросов
#Выведем корреляции между параметрами
df.corr()


# In[89]:


# Посмотрим на средние и медианные знчения датасета после выбросов 
mean_and_50 = df.describe()
mean_and_50.loc[['mean', '50%']]
# Убедимся, что после удаления выбросов среднее и медианное значение остались в пределах предыдущих значений


# # 5	Проведём нормализацию и стандартизацию (продолжим предобработку данных)

# In[90]:


df_norm = df.copy()


# In[91]:


df_norm.info()


# ###
# Нормализуем данные
# 
# У нас в основном количественные признаки, поэтому можно применить нормализацию (приведение в диапазон от 0 до 1) или стандартизацию (приведение к матожиданию 0, стандартному отклонению 1). Т.к. это в том числе учебная работа, то используем и нормализацию, и стандартизацию.
# 
# Этап предобработки данных нужен нам и для введенных данных в будущем приложении, которое явится результатом нашей работы.

# In[92]:


# На данный момент в нашем датасете всего 922 строки.Все имеют int или float. 
fig, ax = plt.subplots(figsize = (12, 6))
df_norm.plot(kind = 'kde', ax = ax)
# Оценка плотности ядра показывает, что наши данные находятся в разных диапазонах.
#А в связи с тем, что диапазоны очень разные, данные нужно нормализовать. Можем приступать к нормализации данных


# ## 5.2 Нормализуем данные с помощью MinMaxScaler() 

# In[93]:


scaler = preprocessing.MinMaxScaler()
col = df.columns
result = scaler.fit_transform(df)

df_minmax_n = pd.DataFrame(result, columns = col)
df_minmax_n.describe()


# In[94]:


plt.figure(figsize = (16,10))
ax = sns.boxplot(data = df_minmax_n)
ax.set_xticklabels(ax.get_xticklabels(),rotation=30);


# In[95]:


fig, ax = plt.subplots(figsize = (12, 6))
df_minmax_n.plot(kind = 'kde', ax = ax)


# In[96]:


sns.pairplot(df_minmax_n, hue = 'Угол нашивки', markers = ["o", "s"], diag_kind = 'auto', palette = 'YlGn')


# In[97]:


mask = np.triu(df_minmax_n.corr())
f, ax = plt.subplots(figsize = (11, 9))
sns.heatmap(df_minmax_n.corr(), mask = mask, annot = True, square = True, cmap = 'Greens_r')
plt.xticks(rotation = 45, ha = 'right')
plt.show()


# ## 5.3 Нормализуем данные с помощью Normalizer()

# In[98]:


normalizer = Normalizer()
res = normalizer.fit_transform(df)
df_norm_n = pd.DataFrame(res, columns = df.columns)
df_norm_n


# In[99]:


fig, ax = plt.subplots(figsize = (12, 6))
df_norm_n.plot(kind = 'kde', ax = ax)


# ## 5.4	Сравним с данными до нормализации

# In[100]:


df.head(10)


# ## 5.5	Проверим перевод данных из нормализованных в исходные 

# In[101]:


col = df_minmax_n.columns
result_reverse = scaler.inverse_transform(df_minmax_n)
initial_data = pd.DataFrame(result_reverse, columns = col)
initial_data.head(10)


# ## 5.6	Рассмотрим несколько вариантов корреляции между параметрами после нормализации 

# In[102]:


#Первый вариант
df_norm_n[df_norm_n.columns].corr()


# In[103]:


#второй вариант
df_minmax_n[df_minmax_n.columns].corr()


# In[104]:


df_minmax_n


# In[105]:


df_norm_n


# In[106]:


#Построим на "ящики с усами"
scaler = MinMaxScaler()
scaler.fit(df_norm_n)
plt.figure(figsize=(20, 20))
#Выводим "ящики"
plt.boxplot(pd.DataFrame(scaler.transform(df_norm_n)), labels = df_norm_n.columns, patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'g', color = 'y'),medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color="black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()


# In[107]:


#Попытаемся посмотреть на "ящики с усами"
scaler = MinMaxScaler()
scaler.fit(df_minmax_n)
plt.figure(figsize = (20, 20))
#Выводим "ящики"
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df_minmax_n.columns, patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'g', color = 'y'),medianprops = dict(color = 'lime'), whiskerprops = dict(color = "g"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()


# In[108]:


# Визуализация графиков показывает, что нормализация при помощи "Normalizer" дает нам большое количество выбросов


# ## 5.7	Стандартизируем данные

# In[109]:


X1 = df_minmax_n.copy()
X2 = df_norm_n.copy()


# In[110]:


df_std_X1 = preprocessing.StandardScaler().fit(X1)
df_standart_X1 = df_std_X1.transform(X1)
df_standart_1 = pd.DataFrame(df_standart_X1)


# In[111]:


fig, ax = plt.subplots(figsize = (12, 6))
df_standart_1.plot(kind = 'kde', ax = ax)


# In[112]:


df_standart_1


# In[113]:


df_std_X2 = preprocessing.StandardScaler().fit(X2)
df_standart_X2 = df_std_X2.transform(X2)
df_standart_2 = pd.DataFrame(df_standart_X2)


# In[114]:


fig, ax = plt.subplots(figsize = (12, 6))
df_standart_2.plot(kind = 'kde', ax=ax)


# In[115]:


df_standart_2


# In[116]:


mask = np.triu(X1.corr())
f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(X1.corr(), mask=mask, annot=True, square=True, cmap='Greens_r')
plt.xticks(rotation=45, ha='right')
plt.show()


# In[117]:


mask = np.triu(X2.corr())
f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(X2.corr(), mask=mask, annot=True, square=True, cmap='Greens_r')
plt.xticks(rotation=45, ha='right')
plt.show()


# In[118]:


df_norm_n.describe()


# In[119]:


df_minmax_n.describe()


# # 6.	Разработаем и обучим нескольких моделей прогноза прочности при растяжении

# ###
# Данные в нашем итоговом датасете в основном непрерывные и на ум приходит сразу решение данной задачи с использованием регрессионных моделей.
# Но попарные графики рассеивания точек и тепловая карта не дают нам реальной взаимосвязи и возможности прямого прогнозирования, 
# поэтому будем использовать и категориальные подходы к прогнозированию (например, попробуем метод ближайших соседей) 
# или обучение со скрытыми слоями, чтобы выявить дополнительные взаимосвязи.
# 
# Задача ВКР на этом этапе звучит так: "При построении моделей провести поиск гиперпараметров модели с помощью поиска по сетке с перекрестной проверкой, количество блоков равно 10.

# In[120]:


# Сформируем выборки и посмотрим, что получилось
# Прогнозируем прочность при растяжении


# ##### После всех подготовительных работ переходим к процессу создания, обучения моделей. Мы будем использовать в Python библиотеку Scikit-Learn. В качестве базового уровня предскажем медианное значение цели на обучающем наборе для всех примеров в тестовом наборе. В качестве метрики возьмём среднюю абсолютную ошибку (mae) в прогнозах. Для обучения используем 70 % данных, а для тестирования — 30 %

# ## 6.2	Разобьём данные на обучающую и тестовую выборки

# In[121]:


#выделяtv предикторы и целевые переменные
x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(
    df_norm_n.loc[:, df_norm_n.columns != 'Прочность при растяжении, МПа'],
    df[['Прочность при растяжении, МПа']],
       test_size = 0.3,
    random_state = 42)


# In[122]:


# Проверка правильности разбивки
df_norm_n.shape[0] - x_train_1.shape[0] - x_test_1.shape[0]


# In[123]:


x_train_1.head(10)


# In[124]:


y_train_1


# In[125]:


y_test_1


# In[126]:


y_train_1.shape


# In[127]:


#Функция для сравнения результатов предсказаний с моделью, выдающей среднее значение по тестовой выборке
def mean_model(y_test_1):
    return [np.mean(y_test_1) for _ in range(len(y_test_1))]
y_1_pred_mean = mean_model(y_test_1)


# ## 6.5	Построим и визуализируем результат работы метода опорных векторов

# In[128]:


svr = make_pipeline(StandardScaler(), SVR(kernel = 'rbf', C = 500.0, epsilon = 1.0))
#обучаем модель
svr.fit(x_train_1, np.ravel(y_train_1))
#вычисляем коэффициент детерминации
y_pred_svr=svr.predict(x_test_1)
mae_svr = mean_absolute_error(y_pred_svr, y_test_1)
mse_svr_elast = mean_squared_error(y_test_1,y_pred_svr)
print('Support Vector Regression Results Train:') 
print("Test score: {:.2f}".format(svr.score(x_train_1, y_train_1))) # Скор для тренировочной выборки
print('Support Vector Regression Results:')
print('SVR_MAE:', round(mean_absolute_error(y_test_1, y_pred_svr)))
print('SVR_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_svr)))
print('SVR_MSE: {:.2f}'.format(mse_svr_elast))
print("SVR_RMSE: {:.2f}".format (np.sqrt(mse_svr_elast)))
print("Test score: {:.2f}".format(svr.score(x_test_1, y_test_1))) # Скор для тестовой выборки


# In[129]:


#Результаты модели, выдающей среднее значение
mse_lin_elast_mean = mean_squared_error(y_test_1, y_1_pred_mean)
print("MAE for mean target: ", mean_absolute_error(y_test_1, y_1_pred_mean))
print("MSE for mean target: ", mse_lin_elast_mean)
print("RMSE for mean target: ", np.sqrt(mse_lin_elast_mean))


# In[130]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Support Vector Regression")
plt.plot(y_pred_svr, label = "Прогноз", color = "orange")
plt.plot(y_test_1.values, label = "Тест", color = "green")
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);


# ## 6.6	Построим и визуализируем результат работы метода случайного леса

# In[131]:


rfr = RandomForestRegressor(n_estimators=15,max_depth=7, random_state=33)
rfr.fit(x_train_1, y_train_1.values)
y_pred_forest = rfr.predict(x_test_1)
mae_rfr = mean_absolute_error(y_pred_forest, y_test_1)
mse_rfr_elast = mean_squared_error(y_test_1,y_pred_forest)
print('Random Forest Regressor Results Train:')
print("Test score: {:.2f}".format(rfr.score(x_train_1, y_train_1))) # Скор для тренировочной выборки
print('Random Forest Regressor Results:')
print('RF_MAE: ', round(mean_absolute_error(y_test_1, y_pred_forest)))
print('RF_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_forest)))
print('RF_MSE: {:.2f}'.format(mse_rfr_elast))
print("RF_RMSE: {:.2f}".format (np.sqrt(mse_rfr_elast)))
print("Test score: {:.2f}".format(rfr.score(x_test_1, y_test_1))) # Скор для тестовой выборки


# In[132]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Random Forest Regressor")
plt.plot(y_pred_forest, label = "Прогноз", color = "orange")
plt.plot(y_test_1.values, label = "Тест", color = 'darkgreen')
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);


# ## 6.7	Построим и визуализируем результат работы линейной регрессии

# In[133]:


lr = LinearRegression()
lr.fit(x_train_1, y_train_1)
y_pred_lr = lr.predict(x_test_1)
mae_lr = mean_absolute_error(y_pred_lr, y_test_1)
mse_lin_elast = mean_squared_error(y_test_1, y_pred_lr)
print('Linear Regression Results Train:') # Скор для тренировочной выборки
print("Test score: {:.2f}".format(lr.score(x_train_1, y_train_1)))
print('Linear Regression Results:')    
print('lr_MAE: ', round(mean_absolute_error(y_test_1, y_pred_lr)))
print('lr_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_lr)))
print('lr_MSE: {:.2f}'.format(mse_lin_elast))
print("lr_RMSE: {:.2f}".format (np.sqrt(mse_lin_elast)))
print("Test score: {:.2f}".format(lr.score(x_test_1, y_test_1))) # Скор для тестовой выборки


# In[134]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Linear Regression")
plt.plot(y_pred_lr, label="Прогноз", color = 'orange')
plt.plot(y_test_1.values, label = "Тест", color = 'darkgreen')
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);

#Линейная регрессия с задачей справилась в 97 % случаев.


# In[135]:


#Визуализация гистограммы распределения ошибки
error = y_test_1 - y_pred_lr
plt.hist(error, bins = 25, color = "g")
plt.xlabel('Prediction Error')
_ = plt.ylabel('Count')


# ## 6.8.	Построим и визуализируем результат работы метода градиентного бустинга

# In[136]:


gbr = make_pipeline(StandardScaler(), GradientBoostingRegressor())
gbr.fit(x_train_1, np.ravel(y_train_1))
y_pred_gbr = gbr.predict(x_test_1)
mae_gbr = mean_absolute_error(y_pred_gbr, y_test_1)
mse_gbr_elast = mean_squared_error(y_test_1,y_pred_gbr)
print('Gradient Boosting Regressor Results Train:')
print("Test score: {:.2f}".format(gbr.score(x_train_1, y_train_1))) # Скор для тренировочной выборки
print('Gradient Boosting Regressor Results:')
print('GBR_MAE: ', round(mean_absolute_error(y_test_1, y_pred_gbr)))
print('GBR_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_gbr)))
print('GBR_MSE: {:.2f}'.format(mse_gbr_elast))
print("GBR_RMSE: {:.2f}".format (np.sqrt(mse_gbr_elast)))
print("Test score: {:.2f}".format(gbr.score(x_test_1, y_test_1)))# Скор для тестовой выборки


# In[137]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Gradient Boosting Regressor")
plt.plot(y_pred_gbr, label = "Прогноз", color = "orange")
plt.plot(y_test_1.values, label = "Тест", color = "green")
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);
#Градиентный бустинг с задачей справился в 97 % случаев.


# ## 6.9.	Построим и визуализируем результат работы метода К ближайших соседей

# In[138]:


knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(x_train_1, y_train_1)
y_pred_knn = knn.predict(x_test_1)
mae_knr = mean_absolute_error(y_pred_knn, y_test_1)
mse_knn_elast = mean_squared_error(y_test_1,y_pred_knn)
print('K Neighbors Regressor  Results Train:')
print("Test score: {:.2f}".format(knn.score(x_train_1, y_train_1)))# Скор для тренировочной выборки
print('K Neighbors Regressor  Results:')
print('KNN_MAE: ', round(mean_absolute_error(y_test_1, y_pred_knn)))
print('KNN_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_knn)))
print('KNN_MSE: {:.2f}'.format(mse_knn_elast))
print("KNN_RMSE: {:.2f}".format (np.sqrt(mse_knn_elast)))
print("Test score: {:.2f}".format(knn.score(x_test_1, y_test_1)))# Скор для тестовой выборки


# In[139]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения K Neighbors Regressor")
plt.plot(y_pred_knn, label = "Прогноз", color = 'orange')
plt.plot(y_test_1.values, label = "Тест", color = 'darkgreen')
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);


# In[140]:


#Визуализация гистограммы распределения ошибки
error = y_test_1 - y_pred_knn
plt.hist(error, bins = 25, color = "g")
plt.xlabel('Prediction Error')
_ = plt.ylabel('Count')


# ## 6.10.	Построим и визуализируем результат работы метода деревья решений

# In[141]:


dtr = DecisionTreeRegressor()
dtr.fit(x_train_1, y_train_1.values)
y_pred_dtr = dtr.predict(x_test_1)
mae_dtr = mean_absolute_error(y_pred_dtr, y_test_1)
mse_dtr_elast = mean_squared_error(y_test_1,y_pred_dtr)
print('Decision Tree Regressor Results Train:')
print("Test score: {:.2f}".format(knn.score(x_train_1, y_train_1)))# Скор для тренировочной выборки
print('Decision Tree Regressor Results:')
print('DTR_MAE: ', round(mean_absolute_error(y_test_1, y_pred_dtr)))
print('DTR_MSE: {:.2f}'.format(mse_dtr_elast))
print("DTR_RMSE: {:.2f}".format (np.sqrt(mse_dtr_elast)))
print('DTR_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_dtr)))
print("Test score: {:.2f}".format(dtr.score(x_test_1, y_test_1)))# Скор для тестовой выборки


# In[142]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Decision Tree Regressor")
plt.plot(y_pred_dtr, label = "Прогноз", color = 'orange')
plt.plot(y_test_1.values, label = "Тест", color = 'darkgreen')
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);


# ## 6.11.	Построим и визуализируем результат работы стохастического градиентного спуска

# In[143]:


sdg = SGDRegressor()
sdg.fit(x_train_1, y_train_1)
y_pred_sdg = sdg.predict(x_test_1)
mae_sdg = mean_absolute_error(y_pred_sdg, y_test_1)
mse_sdg_elast = mean_squared_error(y_test_1,y_pred_sdg)
print('Stochastic Gradient Descent Regressor Results Train:')
print("Test score: {:.2f}".format(sdg.score(x_train_1, y_train_1)))# Скор для тренировочной выборки
print('Stochastic Gradient Descent Regressor Results:')
print('SGD_MAE: ', round(mean_absolute_error(y_test_1, y_pred_sdg)))
print('SGD_MSE: {:.2f}'.format(mse_sdg_elast))
print("SGD_RMSE: {:.2f}".format (np.sqrt(mse_sdg_elast)))
print('SGD_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_sdg)))
print("Test score: {:.2f}".format(sdg.score(x_test_1, y_test_1)))# Скор для тестовой выборки


# In[144]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Stochastic Gradient Descent Regressor")
plt.plot(y_pred_sdg, label = "Прогноз", color = 'orange')
plt.plot(y_test_1.values, label = "Тест", color = 'darkgreen')
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);


# ## 6.12.	Построим и визуализируем результат работы многослойного перцептрона

# In[145]:


mlp = MLPRegressor(random_state = 1, max_iter = 500)
mlp.fit(x_train_1, y_train_1)
y_pred_mlp = mlp.predict(x_test_1)
mae_mlp = mean_absolute_error(y_pred_mlp, y_test_1)
mse_mlp_elast = mean_squared_error(y_test_1,y_pred_mlp)
print('Multi-layer Perceptron regressor Results Train:')
print("Test score: {:.2f}".format(mlp.score(x_train_1, y_train_1)))# Скор для тренировочной выборки
print('Multi-layer Perceptron regressor Results:')
print('SGD_MAE: ', round(mean_absolute_error(y_test_1, y_pred_mlp)))
print('SGD_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_mlp)))
print('SGD_MSE: {:.2f}'.format(mse_mlp_elast))
print("SGD_RMSE: {:.2f}".format (np.sqrt(mse_mlp_elast)))
print("Test score: {:.2f}".format(mlp.score(x_test_1, y_test_1)))# Скор для тестовой выборки


# In[146]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Multi-layer Perceptron regressor")
plt.plot(y_pred_mlp, label = "Прогноз", color = 'orange')
plt.plot(y_test_1.values, label = "Тест", color = 'darkgreen')
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);


# ## 6.13.	Построим и визуализируем результат работы лассо регрессии

# In[147]:


clf = linear_model.Lasso(alpha=0.1)
clf.fit(x_train_1, y_train_1)
y_pred_clf = clf.predict(x_test_1)
mae_clf = mean_absolute_error(y_pred_clf, y_test_1)
mse_clf_elast = mean_squared_error(y_test_1,y_pred_clf)
print('Lasso regressor Results Train:')
print("Test score: {:.2f}".format(clf.score(x_train_1, y_train_1)))# Скор для тренировочной выборки
print('Lasso regressor Results:')
print('SGD_MAE: ', round(mean_absolute_error(y_test_1, y_pred_clf)))
print('SGD_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_1, y_pred_clf)))
print('SGD_MSE: {:.2f}'.format(mse_clf_elast))
print("SGD_RMSE: {:.2f}".format (np.sqrt(mse_clf_elast)))
print("Test score: {:.2f}".format(clf.score(x_test_1, y_test_1)))# Скор для тестовой выборки


# In[148]:


plt.figure(figsize = (10, 7))
plt.title("Тестовые и прогнозные значения Lasso regressor")
plt.plot(y_pred_clf, label = "Прогноз", color = 'orange')
plt.plot(y_test_1.values, label = "Тест", color = 'darkgreen')
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.legend()
plt.grid(True);


# ## 6.14.	Сравним наши модели по метрике МАЕ

# In[149]:


mae_df = {'Регрессор': ['Support Vector', 'RandomForest', 'Linear Regression', 'GradientBoosting', 'KNeighbors', 'DecisionTree', 'SGD', 'MLP', 'Lasso'], 'MAE': [mae_svr, mae_rfr, mae_lr, mae_gbr, mae_knr, mae_dtr, mae_sdg, mae_mlp, mae_clf]} 

mae_df = pd.DataFrame(mae_df)


# In[150]:


mae_df


# ## 6.15.	Найдём лучшие гиперпараметры для случайного леса

# In[151]:


# В машинном обучении гиперпараметрами называют параметры алгоритмов, значения которых устанавливаются перед запуском процесса обучения. 
# В этом смысле они и отличаются от обычных параметров, вычисляемых в процессе обучения. Гиперпараметры используются для управления процессом обучения.
#Один из способов настройки гиперпараметров состоит в том, чтобы заставить компьютер попробовать все возможные комбинации значений параметров. 
#Для этого используем модуль GridSearchCV из библиотеки Scikit Learn. Попытаемся найти наилучшую комбинацию гиперпараметров для построения классификатора для нашего набора данных.
#Метод GridSearch:
#ПЛЮСЫ: Этот метод получит требуемые оптимальные гиперпараметры.
#МИНУСЫ: Операция является исчерпывающей. Если диапазон или число гиперпараметров велики, то вероятности могут исчисляться миллионами, и на завершение потребуется довольно много времени.


# In[152]:


parametrs = { 'n_estimators': [200, 300],
              'max_depth': [9, 15],
              'max_features': ['auto'],
              'criterion': ['mse'] }
grid = GridSearchCV(estimator = rfr, param_grid = parametrs, cv = 10)
grid.fit(x_train_1, y_train_1)


# In[153]:


grid.best_params_


# In[154]:


#Выводим гиперпараметры для оптимальной модели
print(grid.best_estimator_)
knr_upr = grid.best_estimator_
print(f'R2-score RFR для прочности при растяжении, МПа: {knr_upr.score(x_test_1, y_test_1).round(3)}')


# In[155]:


#подставим оптимальные гиперпараметры в нашу модель случайного леса
rfr_grid = RandomForestRegressor(n_estimators = 200, criterion = 'mse', max_depth = 15, max_features = 'auto')
#Обучаем модель
rfr_grid.fit(x_train_1, y_train_1)

predictions_rfr_grid = rfr_grid.predict(x_test_1)
#Оцениваем точность на тестовом наборе
mae_rfr_grid = mean_absolute_error(predictions_rfr_grid, y_test_1)
mae_rfr_grid


# In[156]:


new_row_in_mae_df = {'Регрессор': 'RandomForest_GridSearchCV', 'MAE': mae_rfr_grid} 

mae_df = mae_df.append(new_row_in_mae_df, ignore_index=True)


# In[157]:


# поиск гипермараметров не дал улучшений для уже имеющейся модели RandomForestRegressor
# вероятно, нужно указывать больше вариаций параметров при работе с GridSearchCV
mae_df


# ## 6.17.	Найдём лучшие гиперпараметры для К ближайших соседей

# In[158]:


knn = KNeighborsRegressor()
knn_params = {'n_neighbors' : range(1, 301, 2), 
          'weights' : ['uniform', 'distance'],
          'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']
          }
#Запустим обучение модели. В качестве оценки модели будем использовать коэффициент детерминации (R^2)
# Если R2<0, это значит, что разработанная модель даёт прогноз даже хуже, чем простое усреднение.
gs = GridSearchCV(knn, knn_params, cv = 10, verbose = 1, n_jobs =-1, scoring = 'r2')
gs.fit(x_train_1, y_train_1)
knn_3 = gs.best_estimator_
gs.best_params_


# In[159]:


#Выводим гиперпараметры для оптимальной модели
print(gs.best_estimator_)
gs1 = gs.best_estimator_
print(f'R2-score KNR для прочности при растяжении, МПа: {gs1.score(x_test_1, y_test_1).round(3)}')


# ## 6.18.	Подставим значения в нашу модель К ближайших соседей

# In[160]:


knn_grid = KNeighborsRegressor(algorithm = 'brute', n_neighbors = 7, weights = 'distance')
#Обучаем модель
knn_grid.fit(x_train_1, y_train_1)

predictions_knn_grid = knn_grid.predict(x_test_1)
#Оцениваем точность на тестовом наборе
mae_knn_grid = mean_absolute_error(predictions_knn_grid, y_test_1)
mae_knn_grid


# In[161]:


new_row_in_mae_df = {'Регрессор': 'KNeighbors_GridSearchCV', 'MAE': mae_knn_grid} 

mae_df = mae_df.append(new_row_in_mae_df, ignore_index=True)
mae_df


# ## 6.19.	Найдём лучшие гиперпараметры метода деревья решений

# In[162]:


criterion = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']
splitter = ['best', 'random']
max_depth = [3,5,7,9,11]
min_samples_leaf = [100,150,200]
min_samples_split = [200,250,300]
max_features = ['auto', 'sqrt', 'log2']
param_grid = {'criterion': criterion,
               'splitter': splitter,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'max_features': max_features}
#Запустим обучение модели. В качестве оценки модели будем использовать коэффициент детерминации (R^2)
# Если R2<0, это значит, что разработанная модель даёт прогноз даже хуже, чем простое усреднение.
gs4 = GridSearchCV(dtr, param_grid, cv = 10, verbose = 1, n_jobs =-1, scoring = 'r2')
gs4.fit(x_train_1, y_train_1)
dtr_3 = gs4.best_estimator_
gs.best_params_


# In[163]:


print(gs4.best_estimator_)
gs1 = gs4.best_estimator_
print(f'R2-score DTR для прочности при растяжении, МПа: {gs4.score(x_test_1, y_test_1).round(3)}')


# ## 6.20.	Подставим значения в нашу модель метода деревья решений

# In[164]:


dtr_grid = DecisionTreeRegressor(criterion = 'poisson', max_depth = 7, max_features = 'auto',
                      min_samples_leaf = 100, min_samples_split = 250)
#Обучаем модель
dtr_grid.fit(x_train_1, y_train_1)

predictions_dtr_grid = dtr_grid.predict(x_test_1)
#Оцениваем точность на тестовом наборе
mae_dtr_grid = mean_absolute_error(predictions_dtr_grid, y_test_1)
mae_dtr_grid


# In[165]:


new_row_in_mae_df = {'Регрессор': 'DecisionTree_GridSearchCV', 'MAE': mae_dtr_grid} 

mae_df = mae_df.append(new_row_in_mae_df, ignore_index = True)
mae_df


# ## 6.21.	Проверим все модели и процессинги и выведем лучшую модель и процессинг

# In[166]:


pipe = Pipeline([('preprocessing', StandardScaler()), ('regressor', SVR())])
param_grid = [
{'regressor': [SVR()], 'preprocessing': [StandardScaler(), MinMaxScaler(), None],
'regressor__gamma': [0.001, 0.01, 0.1, 1, 10, 100],
'regressor__C': [0.001, 0.01, 0.1, 1, 10, 100]},
{'regressor': [RandomForestRegressor(n_estimators = 100)],
'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},
{'regressor': [LinearRegression()], 'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},
{'regressor': [GradientBoostingRegressor()], 'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},
{'regressor': [KNeighborsRegressor()], 'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},
{'regressor': [DecisionTreeRegressor()], 'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},
{'regressor': [SGDRegressor()], 'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},
{'regressor': [MLPRegressor(random_state = 1, max_iter = 500)], 'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},
{'regressor': [linear_model.Lasso(alpha = 0.1)], 'preprocessing':  [StandardScaler(), MinMaxScaler(), None]},]
grid = GridSearchCV(pipe, param_grid, cv = 10)
grid.fit(x_train_1, np.ravel(y_train_1))
print("Наилучшие параметры:\n{}\n".format(grid.best_params_))
print("Наилучшее значение правильности перекрестной проверки: {:.2f}".format(grid.best_score_))
print("Правильность на тестовом наборе: {:.2f}".format(grid.score(x_test_1, y_test_1)))


# In[167]:


print("Наилучшая модель:\n{}".format(grid.best_estimator_))


# In[ ]:




